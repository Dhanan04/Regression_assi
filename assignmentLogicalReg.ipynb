{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2895708-8bb6-45b7-bba8-67bb5a9a36b5",
   "metadata": {},
   "source": [
    "Q1. ans.) \n",
    "\n",
    "Linear regression is a supervised learning algorithm used for predicting a numeric output (dependent variable) based on one or more input features (independent variables). It assumes a linear relationship between the input variables and the target variable. The goal of linear regression is to find the best-fit line (or hyperplane in higher dimensions) that minimizes the difference between the predicted and actual values. \n",
    "\n",
    "Logistic regression is also a supervised learning algorithm, but it's used for binary classification problems, where the goal is to predict one of two possible outcomes. Unlike linear regression, logistic regression predicts the probability that a given input belongs to a particular class. The output of logistic regression is transformed using a logistic (sigmoid) function to ensure it lies between 0 and 1, representing the probability of belonging to the positive class.\n",
    "\n",
    "\n",
    "\n",
    "Consider a scenario where you're building a model to predict whether an email is spam or not. The input features could be related to the content of the email, sender information, etc. The output is binary: either the email is spam (positive class) or it's not (negative class). Logistic regression would be more appropriate here because it can provide a probability score indicating how likely the email is to be spam based on the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11422f1d-651b-4df0-b075-e8318ee05744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09996da-e152-48c0-b0ef-0ae39985e25a",
   "metadata": {},
   "source": [
    "Q2. ans)\n",
    "\n",
    "The cost function used in logistic regression is the logistic loss, also known as the log loss and the formula is \n",
    "\n",
    "J(0) = -1/m sub of i=1 to m [ y^i log(h0(x^(i)))+ ( 1-y^i)log(h0(x^(i)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260dedb-2d6c-476f-ae8e-e6c1b7587562",
   "metadata": {},
   "source": [
    "The goal of optimization in logistic regression is to find the values of 0 that minimize this cost function, effectively improving the model's ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165c31b-f591-4916-ab45-58201b162ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f41d8242-d8e5-41ba-a352-25c2ccdb3969",
   "metadata": {},
   "source": [
    "Q3 ans.)\n",
    "\n",
    "Regularization is a technique used in machine learning, including logistic regression, to prevent overfitting. Overfitting occurs when a model learns the training data so well that it captures noise and random fluctuations, leading to poor generalization on new, unseen data. Regularization aims to strike a balance between fitting the training data closely and maintaining simplicity to improve the model's ability to generalize.\n",
    "\n",
    "There are 2 Technique L1 and L2 Regularization \n",
    "In L1  i,e lasso Regularization, it adds the absolute values of the coefficients to the cost function. The L1 regularization term penalizes large coefficients and tends to drive some coefficients to exactly zero. This results in a sparse model where some features are entirely ignored, effectively performing feature selection.\n",
    "\n",
    "In L2 i,e Ridge regularization, it adds the squared values of the coefficients to the cost function. The L2 regularization term penalizes large coefficients but doesn't drive coefficients to exactly zero. Instead, it encourages all coefficients to be small, which can help prevent extreme parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd260e4-44ed-4707-bafa-19f1598f8e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8618afc4-f0b2-44db-ac8d-da3d4ec016f7",
   "metadata": {},
   "source": [
    "Q4 ans.) \n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation used to evaluate the performance of classification models, including logistic regression models. It illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) as the classification threshold is varied.\n",
    "\n",
    "The ROC curve provides insights into the performance of a classification model across different threshold settings. A good model will have a curve that is closer to the top-left corner of the plot. This is because a higher TPR and a lower FPR indicate better performance. The diagonal line from the bottom-left to the top-right represents a random guess, where the true positive rate and false positive rate are roughly equal.\n",
    "\n",
    "The Area Under the Curve (AUC) is a scalar value that quantifies the overall performance of the model. AUC ranges between 0 and 1, where a higher value indicates better performance.  \n",
    "- AUC = 0.5: Random performance (diagonal line)\n",
    "- AUC > 0.5: Better than random performance.\n",
    "- AUC = 1: Perfect performance (ideal curve along the top-left corner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54c2cd-053b-4816-8820-be160e567c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cac10f87-e93b-4066-8eae-fa86c28ff5a2",
   "metadata": {},
   "source": [
    "Q5 ans.)\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features from the original set of features to improve model performance, reduce overfitting, and enhance model interpretability. In the context of logistic regression, feature selection is important because it can lead to a more parsimonious model with improved generalization. techniques for feature selection in logistic regression are :\n",
    "\n",
    "- Univariate Feature Selection:\n",
    "This approach involves evaluating each feature independently in relation to the target variable using statistical tests like chi-squared test or ANOVA for categorical features, and correlation or t-test for numerical features. Features with the highest test statistic values are selected.\n",
    "\n",
    "- L1 Regularization (Lasso):\n",
    "L1 regularization, can drive some coefficients to exactly zero. This effectively performs automatic feature selection by keeping only the most relevant features. Lasso regression tends to create sparse models and is particularly useful when there are many features and you suspect that not all of them are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8febc58-6b41-4c51-8538-7dbbcc247471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c9b957-abe7-4ab1-baa0-58d390d4df09",
   "metadata": {},
   "source": [
    "Q6 ans.) \n",
    "\n",
    "Handling imbalanced datasets in logistic regression, where one class is significantly more frequent than the other, is crucial to ensure that the model doesn't become biased towards the majority class. When dealing with class imbalance, the model can have difficulty correctly predicting the minority class, which is often of more interest.\n",
    "\n",
    "Technique we can use to counter\n",
    "\n",
    "### Resampling Technique:\n",
    "\n",
    "- Oversampling: \n",
    "Increasing the number of instances in the minority class by duplicating or creating synthetic examples. Techniques like Random Oversampling and Synthetic Minority Over-sampling Technique (SMOTE) generate new samples to balance the classes.\n",
    "- Undersampling:\n",
    "Reducing the number of instances in the majority class by randomly selecting a subset of examples. This may lead to loss of information, so undersampling is often combined with other methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6b4da-05af-4cf3-abcc-b4165ecd6b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68539799-b86c-48b5-b4a5-490fc91ac6a3",
   "metadata": {},
   "source": [
    "Q7 ans.)\n",
    "\n",
    "implementing logistic regression can come with various challenges and issues. few common problems that can arise and their solution are here\n",
    "\n",
    "#### Model Overfitting:\n",
    "Logistic regression can overfit when the model is too complex relative to the amount of data available.\n",
    "\n",
    "- Solution:\n",
    "Regularization techniques like L1 (Lasso) or L2 (Ridge) regularization can help control overfitting by penalizing large coefficients.\n",
    "\n",
    "\n",
    "#### Outliers:\n",
    "Outliers can disproportionately influence the model's coefficients and performance.\n",
    "\n",
    "- Solution:\n",
    "Identify and handle outliers using techniques like Z-score, IQR-based methods, or robust regression techniques.\n",
    "\n",
    "#### Multicollinearity:\n",
    "Multicollinearity occurs when independent variables in the model are highly correlated with each other. This can lead to unstable coefficients and difficulties in interpreting their individual effects.\n",
    "\n",
    "- Solution:\n",
    "By removing one of the correlated variables or by applying dimensionality reduction techniques like Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65da90-b45a-456a-8516-722fe6b871fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14506d-272e-4601-bd31-8ab1d8adf8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
